{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import graphviz\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### global constant variables\n",
    "#------------------------------------------------------------------------\n",
    "# general\n",
    "#------------------------------------------------------------------------                #  number of APs\n",
    "VERBOSE = 1                     # 0 for turning off logging\n",
    "#------------------------------------------------------------------------\n",
    "# stacked auto encoder (sae)\n",
    "#------------------------------------------------------------------------\n",
    "# SAE_ACTIVATION = 'tanh'\n",
    "SAE_ACTIVATION = 'relu'\n",
    "SAE_BIAS = False\n",
    "SAE_OPTIMIZER = 'adam'\n",
    "SAE_LOSS = 'mse'\n",
    "#------------------------------------------------------------------------\n",
    "# classifier\n",
    "#------------------------------------------------------------------------\n",
    "CLASSIFIER_ACTIVATION = 'relu'\n",
    "CLASSIFIER_BIAS = False\n",
    "CLASSIFIER_OPTIMIZER = 'adam'\n",
    "CLASSIFIER_LOSS = 'binary_crossentropy'\n",
    "training_ratio = 0.9            # ratio of training data to overall data                # number of labels\n",
    "verbose = 1                     # 0 for turning off logging\n",
    "seed = 7                        # random number seed for reproducibility\n",
    "#------------------------------------------------------------------------\n",
    "# stacked auto encoder (sae)\n",
    "#------------------------------------------------------------------------\n",
    "# sae_activation = 'tanh'\n",
    "sae_activation = 'relu'\n",
    "sae_bias = False\n",
    "sae_optimizer = 'adam'\n",
    "sae_loss = 'mse'\n",
    "#------------------------------------------------------------------------\n",
    "# classifier\n",
    "#------------------------------------------------------------------------\n",
    "# classifier_activation = 'relu'\n",
    "classifier_activation = 'tanh'\n",
    "classifier_bias = False\n",
    "classifier_optimizer = 'adam'\n",
    "# classifier_optimizer = 'rmsprop'\n",
    "classifier_loss = 'categorical_crossentropy'\n",
    "# dropout_rates = [0.5]           # for test\n",
    "# dropout_rates = np.arange(6)*0.1  # 0.0,0.1,...,0.5\n",
    "#------------------------------------------------------------------------\n",
    "# input files\n",
    "#------------------------------------------------------------------------\n",
    "path_train = '../data/UJIIndoorLoc/trainingData2.csv' # '-110' for the lack of AP.\n",
    "path_validation = '../data/UJIIndoorLoc/validationData2.csv' # ditto\n",
    "#------------------------------------------------------------------------\n",
    "# output files\n",
    "#------------------------------------------------------------------------\n",
    "batch_size=10\n",
    "classifier_hidden_layers=[16,16]\n",
    "dropout=0.0\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Beacon 1</th>\n",
       "      <th>Beacon 2</th>\n",
       "      <th>Beacon 3</th>\n",
       "      <th>Beacon 4</th>\n",
       "      <th>Beacon 5</th>\n",
       "      <th>Beacon 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>-66</td>\n",
       "      <td>-72</td>\n",
       "      <td>-73</td>\n",
       "      <td>-67</td>\n",
       "      <td>-77</td>\n",
       "      <td>-83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>-61</td>\n",
       "      <td>-70</td>\n",
       "      <td>-79</td>\n",
       "      <td>-75</td>\n",
       "      <td>-76</td>\n",
       "      <td>-79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-65</td>\n",
       "      <td>-87</td>\n",
       "      <td>-79</td>\n",
       "      <td>-74</td>\n",
       "      <td>-75</td>\n",
       "      <td>-72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>-70</td>\n",
       "      <td>-76</td>\n",
       "      <td>-74</td>\n",
       "      <td>-74</td>\n",
       "      <td>-68</td>\n",
       "      <td>-71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-71</td>\n",
       "      <td>-70</td>\n",
       "      <td>-78</td>\n",
       "      <td>-72</td>\n",
       "      <td>-76</td>\n",
       "      <td>-68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x    y  Beacon 1  Beacon 2  Beacon 3  Beacon 4  Beacon 5  Beacon 6\n",
       "0  2.5  4.5       -66       -72       -73       -67       -77       -83\n",
       "1  3.0  4.5       -61       -70       -79       -75       -76       -79\n",
       "2  3.0  5.0       -65       -87       -79       -74       -75       -72\n",
       "3  3.0  5.5       -70       -76       -74       -74       -68       -71\n",
       "4  3.0  6.0       -71       -70       -78       -72       -76       -68"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_con.csv',\n",
    "            delimiter=',', names=['x', 'y', 'Beacon 1', 'Beacon 2', 'Beacon 3', 'Beacon 4', 'Beacon 5', 'Beacon 6', 'Wifi 1', 'Wifi 2', 'localization1', 'localization4_5g', 'UVIC_Eduroam3', 'localization2', 'localization3_5g', 'UVIC_Eduroam4', 'localization1_5g'])\n",
    "df = df.loc[:,['x','y','Beacon 1', 'Beacon 2', 'Beacon 3', 'Beacon 4', 'Beacon 5', 'Beacon 6']]\n",
    "X_train = df.drop(['x', 'y'], axis = 1)\n",
    "y_train = df[['x', 'y']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Beacon 1</th>\n",
       "      <th>Beacon 2</th>\n",
       "      <th>Beacon 3</th>\n",
       "      <th>Beacon 4</th>\n",
       "      <th>Beacon 5</th>\n",
       "      <th>Beacon 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-73</td>\n",
       "      <td>-67</td>\n",
       "      <td>-79</td>\n",
       "      <td>-66</td>\n",
       "      <td>-52</td>\n",
       "      <td>-71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>-80</td>\n",
       "      <td>-63</td>\n",
       "      <td>-83</td>\n",
       "      <td>-63</td>\n",
       "      <td>-64</td>\n",
       "      <td>-73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-81</td>\n",
       "      <td>-72</td>\n",
       "      <td>-77</td>\n",
       "      <td>-63</td>\n",
       "      <td>-59</td>\n",
       "      <td>-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-74</td>\n",
       "      <td>-69</td>\n",
       "      <td>-79</td>\n",
       "      <td>-64</td>\n",
       "      <td>-63</td>\n",
       "      <td>-74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-72</td>\n",
       "      <td>-82</td>\n",
       "      <td>-85</td>\n",
       "      <td>-58</td>\n",
       "      <td>-62</td>\n",
       "      <td>-67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x    y  Beacon 1  Beacon 2  Beacon 3  Beacon 4  Beacon 5  Beacon 6\n",
       "0  7.0  6.0       -73       -67       -79       -66       -52       -71\n",
       "1  6.0  5.5       -80       -63       -83       -63       -64       -73\n",
       "2  6.5  4.0       -81       -72       -77       -63       -59       -69\n",
       "3  7.0  3.0       -74       -69       -79       -64       -63       -74\n",
       "4  6.0  2.5       -72       -82       -85       -58       -62       -67"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data_test.csv',\n",
    "            delimiter=',', names=['x', 'y', 'Beacon 1', 'Beacon 2', 'Beacon 3', 'Beacon 4', 'Beacon 5', 'Beacon 6', 'Wifi 1', 'Wifi 2', 'localization1', 'localization4_5g', 'UVIC_Eduroam3', 'localization2', 'localization3_5g', 'UVIC_Eduroam4', 'localization1_5g'])\n",
    "df_test = df_test.loc[:,['x','y','Beacon 1', 'Beacon 2', 'Beacon 3', 'Beacon 4', 'Beacon 5', 'Beacon 6']]\n",
    "X_test = df_test.drop(['x', 'y'], axis = 1)\n",
    "y_test = df_test[['x', 'y']]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 22.6411\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4.8699\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3.1658\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.6519\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.4851\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.2543\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.1265\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.0680\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.0216\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.9553\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.9511\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.9044\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.8687\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.8244\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.7400\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.6681\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.6318\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.5583\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.5942\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.5947\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.5075\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.4300\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.4208\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3618\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3600\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3432\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3108\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2901\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3411\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3093\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2945\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2754\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2472\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2503\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2509\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2238\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2161\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2502\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2915\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2097\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2573\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1979\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.2192\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.2207\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.1785\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1383\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1608\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2349\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2100\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2606\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1719\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2172\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1280\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1539\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1836\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2026\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2837\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1459\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1257\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1353\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0941\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1142\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1155\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2050\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1020\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1028\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1633\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1732\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1060\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1425\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1352\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.0924\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1307\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1682\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1235\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0919\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0954\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1736\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2136\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1313\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0848\n",
      "Epoch 82/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1025\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1191\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1666\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1318\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1523\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1176\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0820\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0557\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0792\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0618\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0879\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0688\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0919\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0868\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0705\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0835\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0731\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0876\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0576\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0826\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1180\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.0757\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0727\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0671\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1576\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0751\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1313\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0489\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0525\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0449\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0587\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1360\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0313\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0614\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0722\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1010\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0357\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0103\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1071\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0534\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0784\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0488\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0254\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0861\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0623\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1431\n",
      "Epoch 128/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.0516\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0036\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0129\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0337\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0408\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0184\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0202\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0637\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0524\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0341\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0393\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0450\n",
      "Epoch 140/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0605\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0142\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0110\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0241\n",
      "Epoch 144/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0323\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0491\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0195\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0536\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1045\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0123\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0233\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0163\n",
      "Epoch 152/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2266\n",
      "Epoch 153/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0167\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9804\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9995\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9873\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0324\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0427\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0491\n",
      "Epoch 160/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0925\n",
      "Epoch 161/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0317\n",
      "Epoch 162/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9758\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0122\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0270\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0728\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0439\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0184\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0679\n",
      "Epoch 170/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1041\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0897\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0193\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0141\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0291\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0020\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9667\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0507\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0461\n",
      "Epoch 179/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0106\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0132\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9555\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0145\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0711\n",
      "Epoch 184/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.9893\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0510\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9796\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9748\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0197\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0006\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0148\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9770\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0074\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0019\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9802\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9960\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0076\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9598\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9627\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0065\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0299\n",
      "Epoch 201/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9932\n",
      "Epoch 202/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9831\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0605\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0684\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9760\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0163\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0047\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.9599\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9943\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9595\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0079\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0260\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9918\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9673\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9557\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0102\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9623\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9860\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0041\n",
      "Epoch 220/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9849\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9324\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9573\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9508\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9618\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9582\n",
      "Epoch 226/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.9876\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0421\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9736\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9611\n",
      "Epoch 230/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9917\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9720\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0252\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0192\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9333\n",
      "Epoch 235/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9387\n",
      "Epoch 236/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9567\n",
      "Epoch 237/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9947\n",
      "Epoch 238/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9670\n",
      "Epoch 239/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9448\n",
      "Epoch 240/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9475\n",
      "Epoch 241/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9694\n",
      "Epoch 242/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0536\n",
      "Epoch 243/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9894\n",
      "Epoch 244/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9607\n",
      "Epoch 245/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0451\n",
      "Epoch 246/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0540\n",
      "Epoch 247/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9570\n",
      "Epoch 248/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9594\n",
      "Epoch 249/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9260\n",
      "Epoch 250/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0289\n",
      "Epoch 251/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9664\n",
      "Epoch 252/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9996\n",
      "Epoch 253/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9837\n",
      "Epoch 254/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9337\n",
      "Epoch 255/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9447\n",
      "Epoch 256/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0142\n",
      "Epoch 257/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9696\n",
      "Epoch 258/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9358\n",
      "Epoch 259/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9346\n",
      "Epoch 260/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9442\n",
      "Epoch 261/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.9058\n",
      "Epoch 262/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9257\n",
      "Epoch 263/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0225\n",
      "Epoch 264/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9798\n",
      "Epoch 265/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9310\n",
      "Epoch 266/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9328\n",
      "Epoch 267/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9324\n",
      "Epoch 268/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9270\n",
      "Epoch 269/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9181\n",
      "Epoch 270/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9475\n",
      "Epoch 271/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9259\n",
      "Epoch 272/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0039\n",
      "Epoch 273/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9107\n",
      "Epoch 274/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9432\n",
      "Epoch 275/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9385\n",
      "Epoch 276/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9392\n",
      "Epoch 277/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9458\n",
      "Epoch 278/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9184\n",
      "Epoch 279/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9067\n",
      "Epoch 280/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.9222\n",
      "Epoch 281/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9038\n",
      "Epoch 282/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8962\n",
      "Epoch 283/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9500\n",
      "Epoch 284/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9252\n",
      "Epoch 285/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9239\n",
      "Epoch 286/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9678\n",
      "Epoch 287/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9805\n",
      "Epoch 288/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9001\n",
      "Epoch 289/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9247\n",
      "Epoch 290/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9243\n",
      "Epoch 291/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9165\n",
      "Epoch 292/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9524\n",
      "Epoch 293/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9673\n",
      "Epoch 294/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9199\n",
      "Epoch 295/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9673\n",
      "Epoch 296/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9094\n",
      "Epoch 297/300\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.9124\n",
      "Epoch 298/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9428\n",
      "Epoch 299/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8943\n",
      "Epoch 300/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8704\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Mean Absolute Error: 0.81527664926317\n",
      "Mean Squared Error: 1.1458604396780805\n",
      "Root Mean Squared Error: 1.0704487095036737\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# regressor = DecisionTreeRegressor()\n",
    "# regressor.fit(X_train, y_train)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "input_dim = len(X_train.columns)\n",
    "output_dim = len(y_train.columns)\n",
    "epochs=300\n",
    "sae_hidden_layers=[12,14,10,8,4]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Dense(\n",
    "        sae_hidden_layers[0],\n",
    "        input_dim=input_dim,\n",
    "        activation=SAE_ACTIVATION,\n",
    "        use_bias=SAE_BIAS))\n",
    "for units in sae_hidden_layers[1:]:\n",
    "    model.add(Dense(units, activation=SAE_ACTIVATION, use_bias=SAE_BIAS))\n",
    "model.add(Dense(output_dim, activation=SAE_ACTIVATION, use_bias=SAE_BIAS))\n",
    "model.compile(optimizer=SAE_OPTIMIZER, loss=SAE_LOSS)\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=VERBOSE)\n",
    "\n",
    "regressor = model\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# def plot_loss (history, model_name):\n",
    "#     plt.figure(figsize = (10, 6))\n",
    "#     plt.plot(history.history['loss'])\n",
    "#     plt.plot(history.history['val_loss'])\n",
    "#     plt.title('Model Train vs Validation Loss for ' + model_name)\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['Train loss', 'Validation loss'], loc='upper right')\n",
    "#     #plt.savefig('C:/Users/nious/Documents/Medium/LSTM&GRU/loss_'+model_name+'.jpg', format='jpg', dpi=1000)\n",
    "\n",
    "\n",
    "# plot_loss (history, 'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.8206587 4.6112523]\n",
      " [6.5179462 4.772414 ]\n",
      " [7.0804095 4.472018 ]\n",
      " [6.4103284 4.805838 ]\n",
      " [6.2019362 2.9017863]\n",
      " [5.7595935 5.337904 ]\n",
      " [3.1831985 4.6662393]\n",
      " [6.037645  6.1181264]\n",
      " [3.2642903 4.3584223]]\n",
      "     x    y\n",
      "0  7.0  6.0\n",
      "1  6.0  5.5\n",
      "2  6.5  4.0\n",
      "3  7.0  3.0\n",
      "4  6.0  2.5\n",
      "5  4.5  4.5\n",
      "6  4.0  5.5\n",
      "7  3.0  6.0\n",
      "8  2.5  4.5\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.81527664926317\n",
      "Mean Squared Error: 1.1458604396780805\n",
      "Root Mean Squared Error: 1.0704487095036737\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.Actual.index, df.Actual)\n",
    "plt.plot(df.Actual.index, df.Predicted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
